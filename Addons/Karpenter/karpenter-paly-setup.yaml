---
 - name: Karpenter Setup Playbook
   hosts: localhost
   gather_facts: false
   vars_prompt:
   - name: AWS_ACCOUNT_ID
     prompt: "Enter AWS account ID"
     private: no

   - name: Region
     prompt: "Enter your Region"
     private: no

   - name: Cluster
     prompt: "Enter your Cluster name"
     private: no

   - name: KARPENTER_VERSION
     prompt: "Enter Karpenter Version"
     private: no


   tasks:
   
   - name: IAM Policy for Karpenter controller
     shell: |
             echo '{
                 "Version": "2012-10-17",
                 "Statement": [
                     {
                          "Effect": "Allow",
                          "Principal": {
                              "Service": "ec2.amazonaws.com"
                          },
                          "Action": "sts:AssumeRole"
                     }
                 ]
             }' > /tmp/node-trust-policy.json
             aws iam create-role --role-name "KarpenterNodeRole-{{Cluster}}" --assume-role-policy-document file:///tmp/node-trust-policy.json
     tags: "1"


   - name: Query EKS Cluster OIDC
     shell: |
             aws eks describe-cluster --name {{Cluster}} --region {{Region}} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5
     register: oidc
   - debug:
       msg: "{{ oidc.stdout_lines[0] }}"
     tags: "2"


   - name: Create an IAM OIDC identity provider If not
     shell: |
             eksctl utils associate-iam-oidc-provider --region {{Region}} --cluster {{Cluster}} --approve
     tags: "3"


   - name: Attach required policies for Karpenter role
     shell: |
             aws iam attach-role-policy --role-name "KarpenterNodeRole-{{Cluster}}" --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
             aws iam attach-role-policy --role-name "KarpenterNodeRole-{{Cluster}}" --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
             aws iam attach-role-policy --role-name "KarpenterNodeRole-{{Cluster}}" --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
             aws iam attach-role-policy --role-name "KarpenterNodeRole-{{Cluster}}" --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
     tags: "4"


   - name: Attach the IAM role to an EC2 instance profile
     shell: |
             aws iam create-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-{{Cluster}}"
             aws iam add-role-to-instance-profile --instance-profile-name "KarpenterNodeInstanceProfile-{{Cluster}}" --role-name "KarpenterNodeRole-{{Cluster}}"
     tags: "5"


   - name: Karpenter controller trust policy
     shell: |
             cat << EOF > /tmp/controller-trust-policy.json
             {
                 "Version": "2012-10-17",
                 "Statement": [
                     {
                         "Effect": "Allow",
                         "Principal": {
                             "Federated": "arn:aws:iam::{{AWS_ACCOUNT_ID}}:oidc-provider/oidc.eks.{{Region}}.amazonaws.com/id/{{oidc.stdout_lines[0]}}"
                         },
                         "Action": "sts:AssumeRoleWithWebIdentity",
                         "Condition": {
                             "StringEquals": {
                                 "oidc.eks.{{Region}}.amazonaws.com/id/{{oidc.stdout_lines[0]}}:aud": "sts.amazonaws.com",
                                 "oidc.eks.{{Region}}.amazonaws.com/id/{{oidc.stdout_lines[0]}}:sub": "system:serviceaccount:karpenter:karpenter"
                             }
                         }
                     }
                 ]
             }
             EOF
             aws iam create-role --role-name KarpenterControllerRole-{{Cluster}} --assume-role-policy-document file:///tmp/controller-trust-policy.json
             cat << EOF > /tmp/controller-policy.json
             {
                 "Statement": [
                     {
                         "Action": [
                             "ssm:GetParameter",
                             "ec2:DescribeImages",
                             "ec2:RunInstances",
                             "ec2:DescribeSubnets",
                             "ec2:DescribeSecurityGroups",
                             "ec2:DescribeLaunchTemplates",
                             "ec2:DescribeInstances",
                             "ec2:DescribeInstanceTypes",
                             "ec2:DescribeInstanceTypeOfferings",
                             "ec2:DescribeAvailabilityZones",
                             "ec2:DeleteLaunchTemplate",
                             "ec2:CreateTags",
                             "ec2:CreateLaunchTemplate",
                             "ec2:CreateFleet",
                             "ec2:DescribeSpotPriceHistory",
                             "pricing:GetProducts"
                         ],
                         "Effect": "Allow",
                         "Resource": "*",
                         "Sid": "Karpenter"
                     },
                     {
                         "Action": "ec2:TerminateInstances",
                         "Condition": {
                             "StringLike": {
                                 "ec2:ResourceTag/karpenter.sh/provisioner-name": "*"
                             }
                         },
                         "Effect": "Allow",
                         "Resource": "*",
                         "Sid": "ConditionalEC2Termination"
                     },
                     {
                         "Effect": "Allow",
                         "Action": "iam:PassRole",
                         "Resource": "arn:aws:iam::{{AWS_ACCOUNT_ID}}:role/KarpenterNodeRole-{{Cluster}}",
                         "Sid": "PassNodeIAMRole"
                     },
                     {
                         "Effect": "Allow",
                         "Action": "eks:DescribeCluster",
                         "Resource": "arn:aws:eks:{{Region}}:{{AWS_ACCOUNT_ID}}:cluster/{{Cluster}}",
                         "Sid": "EKSClusterEndpointLookup"
                     }
                 ],
                 "Version": "2012-10-17"
             }
             EOF
             aws iam put-role-policy --role-name KarpenterControllerRole-{{Cluster}} --policy-name KarpenterControllerPolicy-{{Cluster}} --policy-document file:///tmp/controller-policy.json
     tags: "6"
   

  
   - name: Add karpenter tags to subnets
     shell: |
             for NODEGROUP in $(aws eks list-nodegroups --cluster-name {{Cluster}} --region {{Region}} --query 'nodegroups' --output text); do aws ec2 create-tags --tags "Key=karpenter.sh/discovery,Value={{Cluster}}" --region {{Region}} --resources $(aws eks describe-nodegroup --cluster-name {{Cluster}} --region {{Region}} --nodegroup-name $NODEGROUP --query 'nodegroup.subnets' --output text )
             done
     tags: "7"


   - name: Query EKS cluster security group ID
     shell: |
             aws eks describe-cluster --name {{Cluster}} --region {{Region}} --query "cluster.resourcesVpcConfig.clusterSecurityGroupId" --output text
     register: sg
   - debug:
       msg: "{{ sg.stdout_lines[0] }}"
     tags: "8"


   - name: Add karpenter tags to security groups
     shell: |
             aws ec2 create-tags --tags "Key=karpenter.sh/discovery,Value={{Cluster}}" --region {{Region}} --resources {{sg.stdout_lines[0]}}
     tags: "9"


   - name: Query nodegroup for EKS cluster
     shell: |
             aws eks list-nodegroups --cluster-name {{Cluster}} --region {{Region}} --query 'nodegroups' --output text
     register: ng
   - debug: 
       msg: "{{ ng.stdout_lines }}"
     tags: "10"
           

   - name: Creating Karpenter namespace and Install CRD's
     shell: |
             kubectl create namespace karpenter
             kubectl create -f https://raw.githubusercontent.com/aws/karpenter/{{KARPENTER_VERSION}}/pkg/apis/crds/karpenter.sh_provisioners.yaml
             kubectl create -f https://raw.githubusercontent.com/aws/karpenter/{{KARPENTER_VERSION}}/pkg/apis/crds/karpenter.k8s.aws_awsnodetemplates.yaml
             kubectl create -f https://raw.githubusercontent.com/aws/karpenter/{{KARPENTER_VERSION}}/pkg/apis/crds/karpenter.sh_machines.yaml
     tags: "11"


   - name: Download Helm chart for Karpenter
     shell: |
             helm template karpenter oci://public.ecr.aws/karpenter/karpenter --version {{KARPENTER_VERSION}} --namespace karpenter \
             --set settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-{{Cluster}} \
             --set settings.aws.clusterName={{Cluster}} \
             --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::{{AWS_ACCOUNT_ID}}:role/KarpenterControllerRole-{{Cluster}}" \
             --set controller.resources.requests.cpu=1 \
             --set controller.resources.requests.memory=1Gi \
             --set controller.resources.limits.cpu=1 \
             --set controller.resources.limits.memory=1Gi > /tmp/karpenter.yaml
     tags: "12"


   - name: Create provisioner for karpenter
     shell: |
             cat <<EOF | kubectl apply -f -
             apiVersion: karpenter.sh/v1alpha5
             kind: Provisioner
             metadata:
               name: default
             spec:
               requirements:
               ########### Instance Types Like T2 Family t2.medium, t2.large  ##############
               - key: karpenter.k8s.aws/instance-category
                 operator: In
                 values: [t]

               - key: karpenter.k8s.aws/instance-generation
                 operator: Gt
                 values: ["2"]

               #- key: "topology.kubernetes.io/zone"
               #  operator: In
               #  values: ["us-west-2a", "us-west-2b"]

               - key: "kubernetes.io/arch"
                 operator: In
                 values: ["amd64"]

               ##### If not included, the webhook for the AWS cloud provider will default to on-demand ####
               - key: "karpenter.sh/capacity-type"
                 operator: In
                 values: ["spot"]
                 #values: ["spot", "on-demand"]

               #limits:
               #  resources:
               #    cpu: 1000
               #    memory: 1Gi
               # Enables consolidation which attempts to reduce cluster cost by both removing un-needed nodes and down-sizing
               # Mutually exclusive with the ttlSecondsAfterEmpty parameter.
               #consolidation:
               #  enabled: true

               providerRef:
                 name: default
               ttlSecondsAfterEmpty: 30
             ---
             apiVersion: karpenter.k8s.aws/v1alpha1
             kind: AWSNodeTemplate
             metadata:
               name: default
             spec:
               subnetSelector:
                 karpenter.sh/discovery: "{{Cluster}}"
               securityGroupSelector:
                 karpenter.sh/discovery: "{{Cluster}}"
             EOF
     tags: "13"


   - name: Set node affinity for karpenter deployment
     shell: |
             echo "Modify the affinity so karpenter will run on one of the existing node group nodes
             #### vi /tmp/karpenter.yaml ####
             - matchExpressions:
               - key: eks.amazonaws.com/nodegroup
                 operator: In
                 values:
                 - {{ng.stdout_lines[0]}}"
     register: outs
   - debug: var=outs.stdout_lines
     tags: "14"


   - name: Update aws-auth configMap
     vars:
       EC2PrivateDNSName: EC2PrivateDNSName
     shell: |
             echo "Update aws-auth configMap with below values
             ### kubectl edit configmap aws-auth -n kube-system ###
             - groups:
               - system:bootstrappers
               - system:nodes
               rolearn: arn:aws:iam::{{AWS_ACCOUNT_ID}}:role/KarpenterNodeRole-{{Cluster}}
               username: system:node:{{EC2PrivateDNSName}}"
     register: outs
   - debug: var=outs.stdout_lines
     tags: "15"


   - name: Deploy karpenter
     shell: |
             echo "Karepenter helm values downloaded under /tmp/karepenter.yaml Please do above node affinity and aws-auth configMap update and deploy ### kubectl apply -f /tmp/karpenter.yaml"
     register: outs
   - debug: var=outs.stdout_lines
     tags: "16"


   - name: Setup Prometheus and Grafana with Karpenter helm values
     shell: |
             helm repo add grafana-charts https://grafana.github.io/helm-charts
             helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
             helm repo update
             kubectl create namespace monitoring
             curl -fsSL https://raw.githubusercontent.com/aws/karpenter/{{KARPENTER_VERSION}}/website/content/en/preview/getting-started/getting-started-with-karpenter/prometheus-values.yaml | tee /tmp/prometheus-values.yaml
             curl -fsSL https://raw.githubusercontent.com/aws/karpenter/{{KARPENTER_VERSION}}/website/content/en/preview/getting-started/getting-started-with-karpenter/grafana-values.yaml | tee /tmp/grafana-values.yaml
     tags: "17"


   - name: Deploy Prometheus and Grafana after karpenter deployed
     shell: |
             echo "helm install --namespace monitoring prometheus prometheus-community/prometheus --values /tmp/prometheus-values.yaml
             helm install --namespace monitoring grafana grafana-charts/grafana --values /tmp/grafana-values.yaml"
     register: outs
   - debug: var=outs.stdout_lines
     tags: "18"
